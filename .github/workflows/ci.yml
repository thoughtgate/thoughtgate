name: CI/CD Pipeline

on:
  push:
    branches:
      - main
    tags:
      - 'v*'
  pull_request:
    branches:
      - '**'

# Cancel in-progress runs when a new commit is pushed to the same PR/branch
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

env:
  CARGO_TERM_COLOR: always
  RUST_BACKTRACE: 1

jobs:
  # Stage 0: Detect if only docs/config changed (skip heavy jobs)
  changes:
    name: Detect Changes
    runs-on: ubuntu-latest
    outputs:
      code: ${{ steps.filter.outputs.code }}
    steps:
      - uses: actions/checkout@v6
      - uses: dorny/paths-filter@v3
        id: filter
        with:
          filters: |
            code:
              - 'src/**'
              - 'tests/**'
              - 'benches/**'
              - 'Cargo.toml'
              - 'Cargo.lock'
              - 'Dockerfile'
              - 'build.rs'
              - '.cargo/**'

  # Stage 1a: Quick Checks (fast-fail)
  quick-checks:
    name: Format & Lint
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Install Rust toolchain
        uses: dtolnay/rust-toolchain@stable
        with:
          components: clippy, rustfmt

      - name: Cache cargo registry
        uses: Swatinem/rust-cache@v2
        with:
          shared-key: "checks"
          cache-on-failure: true

      - name: Install cargo-binstall
        uses: cargo-bins/cargo-binstall@main

      - name: Install cargo-audit
        run: |
          # Force reinstall to handle cached but missing binaries
          cargo binstall --no-confirm --force cargo-audit || cargo install --force cargo-audit
          # Verify installation
          cargo audit --version

      - name: Check formatting
        run: cargo fmt -- --check

      - name: Run clippy
        run: cargo clippy --all-targets --all-features -- -D warnings

      - name: Security audit
        run: cargo audit --deny warnings

  # Stage 1b: Build Binaries
  build:
    name: Build Binaries
    runs-on: ubuntu-latest
    needs: quick-checks
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Install Rust toolchain
        uses: dtolnay/rust-toolchain@stable

      - name: Cache cargo registry and build artifacts
        uses: Swatinem/rust-cache@v2
        with:
          shared-key: "build"
          cache-on-failure: true

      - name: Build release binaries
        run: |
          cargo build --release --bin thoughtgate
          cargo build --release --bin mock_llm --features mock

      - name: Upload binaries as artifact
        uses: actions/upload-artifact@v6
        with:
          name: binaries
          path: |
            target/release/thoughtgate
            target/release/mock_llm
          retention-days: 7

  # Stage 1c: Test Suite (parallel execution)
  test:
    name: Test (${{ matrix.test-name }})
    runs-on: ubuntu-latest
    needs: quick-checks
    
    strategy:
      fail-fast: false
      matrix:
        include:
          - test-name: "unit"
            test-cmd: "cargo test --lib -- --nocapture"
          - test-name: "streaming"
            test-cmd: "cargo test --test integration_streaming -- --nocapture"
          - test-name: "peeking"
            test-cmd: "cargo test --test unit_peeking -- --nocapture"
          - test-name: "memory"
            test-cmd: "cargo test --test memory_profile -- --nocapture"
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Install Rust toolchain
        uses: dtolnay/rust-toolchain@stable

      - name: Cache cargo registry and build artifacts
        uses: Swatinem/rust-cache@v2
        with:
          shared-key: "test-${{ matrix.test-name }}"
          cache-on-failure: true

      - name: Run ${{ matrix.test-name }} tests
        run: ${{ matrix.test-cmd }}

  # NOTE: Fuzzing has been moved to nightly-fuzz.yml workflow
  # It runs at 2 AM UTC daily on the main branch only
  # This keeps CI fast while still providing comprehensive fuzz coverage

  # Stage 1d: Benchmarks (Performance Tracking with Bencher.dev)
  # Only runs on main branch and release tags to save CI time on PRs
  #
  # Implements: REQ-OBS-001 (Performance Metrics & Benchmarking)
  #
  # This job runs Criterion benchmarks and reports to Bencher.dev for:
  # - Statistical regression detection
  # - Branch-aware tracking (main/releases)
  # - Marketing-ready performance dashboards
  bench:
    name: Benchmarks
    runs-on: ubuntu-latest
    needs: quick-checks
    if: github.event_name == 'push'  # Only on main/tags, skip PRs
    continue-on-error: true  # Allow CI to pass while new baselines are established
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Install Rust toolchain
        uses: dtolnay/rust-toolchain@stable

      - name: Cache cargo registry and build artifacts
        uses: Swatinem/rust-cache@v2
        with:
          shared-key: "bench"
          cache-on-failure: true

      - name: Build release binaries
        run: cargo build --release --bin thoughtgate --bin mock_mcp --features mock

      - name: Run Criterion benchmarks
        run: |
          echo "Running Criterion benchmarks..."
          cargo bench --bench ttfb -- --noplot
          cargo bench --bench policy_eval -- --noplot || echo "policy_eval benchmark not yet available"
          
          echo ""
          echo "Benchmark complete. Checking output..."
          ls -la target/criterion/ || echo "Warning: target/criterion not created"

      - name: Collect binary metrics
        run: |
          echo "Collecting binary size metrics..."
          ./scripts/collect_metrics.sh --binary-only --output binary_metrics.json
          cat binary_metrics.json

      - name: Collect startup metrics
        run: |
          echo "Collecting startup timing metrics (M-START-001, M-START-003, M-MEM-001)..."
          START_NS=$(date +%s%N)

          # ThoughtGate v0.2 uses 3-port Envoy-style model:
          # - Outbound port (18080): Main proxy for client requests
          # - Admin port (18082): Health checks (/health, /ready)
          # Note: Inbound port (18081) is reserved but not wired yet
          THOUGHTGATE_UPSTREAM_URL="http://127.0.0.1:19999" \
          THOUGHTGATE_OUTBOUND_PORT=18080 \
          THOUGHTGATE_ADMIN_PORT=18082 \
          ./target/release/thoughtgate &
          PID=$!

          # Wait for /health endpoint on admin port
          MAX_WAIT=100
          WAITED=0
          while ! curl -sf http://127.0.0.1:18082/health > /dev/null 2>&1; do
            sleep 0.1
            WAITED=$((WAITED + 1))
            if [ "$WAITED" -ge "$MAX_WAIT" ]; then
              echo "ERROR: Health check timed out"
              kill $PID 2>/dev/null || true
              exit 1
            fi
          done
          HEALTHY_NS=$(date +%s%N)
          STARTUP_HEALTHY_MS=$(( (HEALTHY_NS - START_NS) / 1000000 ))
          echo "Time to healthy: ${STARTUP_HEALTHY_MS}ms"

          # Wait for /ready endpoint on admin port
          WAITED=0
          STARTUP_READY_MS=$STARTUP_HEALTHY_MS
          while ! curl -sf http://127.0.0.1:18082/ready > /dev/null 2>&1; do
            sleep 0.1
            WAITED=$((WAITED + 1))
            if [ "$WAITED" -ge "$MAX_WAIT" ]; then
              echo "WARN: Readiness check timed out"
              break
            fi
          done
          if [ "$WAITED" -lt "$MAX_WAIT" ]; then
            READY_NS=$(date +%s%N)
            STARTUP_READY_MS=$(( (READY_NS - START_NS) / 1000000 ))
          fi
          echo "Time to ready: ${STARTUP_READY_MS}ms"

          # Collect idle memory
          IDLE_RSS=$(grep VmRSS /proc/$PID/status 2>/dev/null | awk '{print $2 * 1024}' || echo "0")
          echo "Idle RSS: ${IDLE_RSS} bytes"

          kill $PID 2>/dev/null || true
          wait $PID 2>/dev/null || true

          cat > startup_metrics.json << EOF
          [
            {"name": "startup/to_healthy", "value": $STARTUP_HEALTHY_MS, "unit": "ms"},
            {"name": "startup/to_ready", "value": $STARTUP_READY_MS, "unit": "ms"},
            {"name": "memory/idle_rss", "value": $IDLE_RSS, "unit": "bytes"}
          ]
          EOF
          cat startup_metrics.json

      - name: Install k6
        run: |
          curl -sL https://github.com/grafana/k6/releases/download/v0.54.0/k6-v0.54.0-linux-amd64.tar.gz | tar xz
          sudo mv k6-v0.54.0-linux-amd64/k6 /usr/local/bin/
          k6 version

      - name: Run k6 latency benchmark
        run: |
          echo "Running k6 latency benchmark with MCP traffic..."

          # Start mock MCP server (zero delay for benchmarking)
          MOCK_MCP_PORT=8888 MOCK_MCP_DELAY_MS=0 ./target/release/mock_mcp &
          MOCK_PID=$!
          sleep 1

          # Start ThoughtGate proxy (uses default ports: 7467 outbound, 7469 admin)
          THOUGHTGATE_UPSTREAM_URL="http://127.0.0.1:8888" \
          ./target/release/thoughtgate &
          PROXY_PID=$!

          # Wait for proxy health on admin port (default: 7469)
          for i in {1..30}; do
            curl -sf http://127.0.0.1:7469/health && break
            sleep 0.5
          done

          # Run k6 benchmark (uses default port 7467)
          k6 run --out json=k6_raw.json tests/benchmark-fast.js || true

          # Cleanup
          kill $PROXY_PID $MOCK_PID 2>/dev/null || true
          wait $PROXY_PID 2>/dev/null || true
          wait $MOCK_PID 2>/dev/null || true

          # Parse k6 results to extract latency percentiles
          python3 << 'PYEOF'
          import json

          durations = []
          try:
              with open("k6_raw.json") as f:
                  for line in f:
                      try:
                          data = json.loads(line.strip())
                          if data.get("type") == "Point" and data.get("metric") == "http_req_duration":
                              value = data.get("data", {}).get("value")
                              if value is not None:
                                  durations.append(value)
                      except:
                          continue
          except FileNotFoundError:
              print("k6_raw.json not found, skipping k6 metrics")

          metrics = {}
          if durations:
              durations.sort()
              n = len(durations)
              metrics["latency/k6_p50"] = {"latency": {"value": durations[n // 2]}}
              metrics["latency/k6_p95"] = {"latency": {"value": durations[int(n * 0.95)]}}
              metrics["latency/k6_p99"] = {"latency": {"value": durations[min(int(n * 0.99), n - 1)]}}
              # 5 second test duration
              metrics["throughput/k6_rps"] = {"throughput": {"value": len(durations) / 5}}
              print(f"Extracted {len(durations)} latency samples from k6")

          with open("k6_metrics.json", "w") as f:
              json.dump(metrics, f, indent=2)
          PYEOF

          cat k6_metrics.json || echo "{}"

      - name: Parse and consolidate benchmark results
        run: |
          echo "Consolidating benchmark results for Bencher..."
          
          python3 << 'PYEOF'
          import json
          from pathlib import Path
          
          # Use dict to deduplicate by name
          # Bencher BMF format: {"benchmark_name": {"measure": {"value": X}}}
          # Three levels: benchmark -> measure -> value object
          metrics = {}
          
          def add_metric(name, value, measure="latency"):
              """Add metric in BMF format with proper measure."""
              metrics[name] = {measure: {"value": value}}
          
          # Parse binary metrics (use "file-size" measure for size metrics)
          binary_file = Path("binary_metrics.json")
          if binary_file.exists():
              with open(binary_file) as f:
                  for m in json.load(f):
                      add_metric(m["name"], m["value"], "file-size")
          
          # Parse startup metrics (use "latency" for timing, "file-size" for memory)
          startup_file = Path("startup_metrics.json")
          if startup_file.exists():
              with open(startup_file) as f:
                  for m in json.load(f):
                      name = m["name"]
                      if "memory" in name or "rss" in name.lower():
                          add_metric(name, m["value"], "file-size")
                      else:
                          add_metric(name, m["value"], "latency")
          
          # Parse Criterion results
          # Only look in "new" directory to avoid duplicates from base/new
          criterion_dir = Path("target/criterion")
          if criterion_dir.exists():
              for estimates_file in criterion_dir.rglob("new/estimates.json"):
                  rel_path = estimates_file.relative_to(criterion_dir)
                  parts = rel_path.parts
                  
                  if len(parts) < 3:
                      continue
                  
                  # Build benchmark name from path (excluding "new" directory)
                  # e.g., policy/eval_simple/new/estimates.json -> policy_eval_simple
                  bench_parts = [p for p in parts[:-2] if p != "new"]
                  bench_name = "_".join(bench_parts).replace(" ", "_")
                  
                  try:
                      with open(estimates_file) as f:
                          data = json.load(f)
                          mean_ns = data["mean"]["point_estimate"]
                          
                          # Convert to appropriate units
                          if "policy" in bench_name.lower():
                              # Policy benchmarks in microseconds
                              add_metric(f"policy/{bench_name}", mean_ns / 1000, "latency")
                          else:
                              # TTFB benchmarks in milliseconds
                              add_metric(f"ttfb/{bench_name}", mean_ns / 1_000_000, "latency")
                  except (KeyError, json.JSONDecodeError) as e:
                      print(f"Warning: Could not parse {estimates_file}: {e}")

          # Parse k6 metrics (already in BMF format)
          k6_file = Path("k6_metrics.json")
          if k6_file.exists():
              try:
                  with open(k6_file) as f:
                      k6_data = json.load(f)
                      metrics.update(k6_data)
                      print(f"Added {len(k6_data)} k6 metrics")
              except (json.JSONDecodeError, FileNotFoundError) as e:
                  print(f"Warning: Could not parse k6_metrics.json: {e}")

          # Write consolidated results in Bencher BMF format
          with open("bench_metrics.json", "w") as f:
              json.dump(metrics, f, indent=2)
          
          print(f"Consolidated {len(metrics)} metrics:")
          for name, data in sorted(metrics.items()):
              measure = list(data.keys())[0]
              print(f"  - {name} [{measure}]: {data[measure]['value']:.3f}")
          PYEOF
          
          echo ""
          echo "=== Final benchmark results ==="
          cat bench_metrics.json

      - name: Install Bencher CLI
        uses: bencherdev/bencher@main

      - name: Report benchmarks to Bencher (main branch)
        if: github.event_name == 'push' && github.ref == 'refs/heads/main'
        run: |
          # REQ-OBS-001: Report with thresholds for regression detection
          # Uses t-test with 25% upper boundary for latency (accommodates CI variance)
          # Uses 15% threshold for file-size (binary/memory metrics)
          # Note: --err removed to prevent job failure on minor regressions
          # Alerts are still created and visible in Bencher dashboard
          bencher run \
            --project thoughtgate \
            --token '${{ secrets.BENCHER_API_TOKEN }}' \
            --branch main \
            --testbed ci-ubuntu-latest \
            --adapter json \
            --file bench_metrics.json \
            --threshold-measure latency \
            --threshold-test t_test \
            --threshold-max-sample-size 64 \
            --threshold-upper-boundary 0.99 \
            --threshold-measure file-size \
            --threshold-test percentage \
            --threshold-upper-boundary 0.15
        env:
          BENCHER_API_TOKEN: ${{ secrets.BENCHER_API_TOKEN }}

      - name: Report benchmarks to Bencher (release tags)
        if: startsWith(github.ref, 'refs/tags/v')
        run: |
          bencher run \
            --project thoughtgate \
            --token '${{ secrets.BENCHER_API_TOKEN }}' \
            --branch releases \
            --testbed ci-ubuntu-latest \
            --adapter json \
            --file bench_metrics.json \
            --branch-reset
        env:
          BENCHER_API_TOKEN: ${{ secrets.BENCHER_API_TOKEN }}

      - name: Upload benchmark artifacts
        if: always()
        uses: actions/upload-artifact@v6
        with:
          name: benchmark-results
          path: |
            bench_metrics.json
            binary_metrics.json
            startup_metrics.json
          if-no-files-found: ignore
          retention-days: 30

  # Stage 2: Build Container Image
  # Skip for docs-only changes on PRs (always run on main/tags)
  build-container:
    name: Build Container Image
    runs-on: ubuntu-latest
    needs: [changes, build, test]
    if: needs.changes.outputs.code == 'true' || github.event_name == 'push'

    outputs:
      image-tag: ${{ steps.meta.outputs.tags }}
      
    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Generate Docker metadata
        id: meta
        run: |
          if [[ "${{ github.ref }}" == refs/tags/v* ]]; then
            VERSION=${GITHUB_REF#refs/tags/}
            echo "tags=thoughtgate:${VERSION},thoughtgate:latest" >> $GITHUB_OUTPUT
            echo "labels=version=${VERSION},commit=${GITHUB_SHA}" >> $GITHUB_OUTPUT
          elif [[ "${{ github.ref }}" == refs/heads/main ]]; then
            echo "tags=thoughtgate:main-${GITHUB_SHA::8}" >> $GITHUB_OUTPUT
            echo "labels=commit=${GITHUB_SHA}" >> $GITHUB_OUTPUT
          else
            PR_NUMBER=${{ github.event.pull_request.number }}
            echo "tags=thoughtgate:pr-${PR_NUMBER}" >> $GITHUB_OUTPUT
            echo "labels=pr=${PR_NUMBER}" >> $GITHUB_OUTPUT
          fi

      - name: Build Docker image (amd64)
        uses: docker/build-push-action@v6
        with:
          context: .
          file: ./Dockerfile
          platforms: linux/amd64
          tags: thoughtgate:test
          cache-from: type=gha
          cache-to: type=gha,mode=max
          outputs: type=docker,dest=/tmp/thoughtgate-image.tar

      - name: Upload Docker image as artifact
        uses: actions/upload-artifact@v6
        with:
          name: docker-image
          path: /tmp/thoughtgate-image.tar
          retention-days: 1

  # Stage 3: Kubernetes Integration Tests
  # Skip for docs-only changes on PRs (always run on main/tags)
  #
  # Implements: REQ-OBS-001 (K8s latency/throughput metrics)
  k8s-tests:
    name: Kubernetes Integration Tests
    runs-on: ubuntu-latest
    needs: [changes, build-container]
    if: needs.changes.outputs.code == 'true' || github.event_name == 'push'
    permissions:
      pull-requests: write  # Needed for Bencher PR comments
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Install Rust toolchain
        uses: dtolnay/rust-toolchain@stable

      - name: Cache cargo registry and build artifacts
        uses: Swatinem/rust-cache@v2
        with:
          cache-on-failure: true

      - name: Download Docker image artifact
        uses: actions/download-artifact@v7
        with:
          name: docker-image
          path: /tmp

      - name: Load Docker image
        run: docker load --input /tmp/thoughtgate-image.tar

      - name: Create Kind cluster
        uses: helm/kind-action@v1.13.0
        with:
          cluster_name: kind
          wait: 30s

      - name: Load image into Kind cluster
        run: kind load docker-image thoughtgate:test --name kind

      - name: Verify image loaded in Kind
        run: |
          docker exec kind-control-plane crictl images | grep thoughtgate || \
          (echo "Failed to load image into Kind" && exit 1)

      - name: Run Kubernetes integration tests
        run: cargo test --test integration_k8s -- --nocapture

      - name: Install Bencher CLI
        if: always()
        uses: bencherdev/bencher@main

      - name: Report K8s metrics to Bencher (main branch)
        if: always() && github.event_name == 'push' && github.ref == 'refs/heads/main'
        run: |
          if [ -f bench_metrics.json ]; then
            # REQ-OBS-001: K8s metrics with t-test (allows for Kind variance)
            # K8s/Kind has higher variance than regular CI, use statistical testing
            bencher run \
              --project thoughtgate \
              --token '${{ secrets.BENCHER_API_TOKEN }}' \
              --branch main \
              --testbed ci-k8s-kind \
              --adapter json \
              --file bench_metrics.json \
              --threshold-measure latency \
              --threshold-test t_test \
              --threshold-max-sample-size 64 \
              --threshold-upper-boundary 0.99 \
              || echo "Bencher report failed (non-fatal)"
          else
            echo "No bench_metrics.json found, skipping Bencher report"
          fi
        env:
          BENCHER_API_TOKEN: ${{ secrets.BENCHER_API_TOKEN }}

      - name: Report K8s metrics to Bencher (Pull Requests)
        if: always() && github.event_name == 'pull_request'
        run: |
          if [ -f bench_metrics.json ]; then
            # REQ-OBS-001: K8s PR checks with t-test
            bencher run \
              --project thoughtgate \
              --token '${{ secrets.BENCHER_API_TOKEN }}' \
              --branch "pr-${{ github.event.pull_request.number }}" \
              --branch-start-point main \
              --testbed ci-k8s-kind \
              --adapter json \
              --file bench_metrics.json \
              --threshold-measure latency \
              --threshold-test t_test \
              --threshold-max-sample-size 64 \
              --threshold-upper-boundary 0.99 \
              --github-actions '${{ secrets.GITHUB_TOKEN }}' \
              || echo "Bencher report failed (non-fatal)"
          else
            echo "No bench_metrics.json found, skipping Bencher report"
          fi
        env:
          BENCHER_API_TOKEN: ${{ secrets.BENCHER_API_TOKEN }}

      - name: Upload performance metrics
        if: always()
        uses: actions/upload-artifact@v6
        with:
          name: k8s-performance-metrics
          path: bench_metrics.json
          if-no-files-found: ignore
          retention-days: 30

      - name: Dump Kind cluster logs on failure
        if: failure()
        run: |
          echo "=== Kind cluster info ==="
          kubectl cluster-info
          echo "=== All pods ==="
          kubectl get pods --all-namespaces
          echo "=== Kind node logs ==="
          kubectl get nodes -o wide
          docker exec kind-control-plane journalctl --no-pager || true

  # Stage 4: Publish Container (Tags Only)
  publish:
    name: Publish Container to GHCR
    runs-on: ubuntu-latest
    needs: k8s-tests
    if: startsWith(github.ref, 'refs/tags/v')
    
    permissions:
      contents: read
      packages: write
      
    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to GitHub Container Registry
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Extract version from tag
        id: version
        run: |
          VERSION=${GITHUB_REF#refs/tags/}
          echo "version=${VERSION}" >> $GITHUB_OUTPUT
          echo "Publishing version: ${VERSION}"

      - name: Generate Docker metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ghcr.io/${{ github.repository }}
          tags: |
            type=semver,pattern={{version}}
            type=semver,pattern={{major}}.{{minor}}
            type=semver,pattern={{major}}
            # Job-level condition ensures tag-only execution
            type=raw,value=latest
          labels: |
            org.opencontainers.image.title=ThoughtGate
            org.opencontainers.image.description=High-performance sidecar proxy for governing MCP and A2A agentic AI traffic
            org.opencontainers.image.vendor=${{ github.repository_owner }}
            org.opencontainers.image.version=${{ steps.version.outputs.version }}

      - name: Build and push multi-arch image
        uses: docker/build-push-action@v6
        with:
          context: .
          file: ./Dockerfile
          platforms: linux/amd64,linux/arm64
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

      # REQ-OBS-001: Collect Docker image metrics (M-BIN-002 to M-BIN-005)
      - name: Collect Docker image metrics
        id: docker_metrics
        run: |
          IMAGE="ghcr.io/${{ github.repository }}:${{ steps.version.outputs.version }}"
          echo "Collecting Docker image metrics for $IMAGE..."
          
          # M-BIN-002: Compressed image size (from manifest)
          # Pull manifest and calculate total compressed size
          MANIFEST=$(docker manifest inspect "$IMAGE" -v 2>/dev/null || echo "[]")
          
          if [ "$MANIFEST" != "[]" ]; then
            # Calculate total compressed size across all layers
            COMPRESSED_SIZE=$(echo "$MANIFEST" | jq '[.[] | .OCIManifest.layers[].size // 0] | add // 0')
            echo "compressed_size=$COMPRESSED_SIZE" >> $GITHUB_OUTPUT
            echo "  Compressed size: $COMPRESSED_SIZE bytes ($(echo "scale=2; $COMPRESSED_SIZE / 1048576" | bc) MB)"
            
            # M-BIN-003: Layer count (from first arch)
            LAYER_COUNT=$(echo "$MANIFEST" | jq '.[0].OCIManifest.layers | length // 0')
            echo "layer_count=$LAYER_COUNT" >> $GITHUB_OUTPUT
            echo "  Layer count: $LAYER_COUNT"
            
            # M-BIN-005: Multi-arch verification
            AMD64=$(echo "$MANIFEST" | jq -e '.[] | select(.Descriptor.platform.architecture == "amd64")' > /dev/null && echo "true" || echo "false")
            ARM64=$(echo "$MANIFEST" | jq -e '.[] | select(.Descriptor.platform.architecture == "arm64")' > /dev/null && echo "true" || echo "false")
            MULTI_ARCH=$([[ "$AMD64" == "true" && "$ARM64" == "true" ]] && echo "true" || echo "false")
            echo "multi_arch=$MULTI_ARCH" >> $GITHUB_OUTPUT
            echo "  Multi-arch (amd64+arm64): $MULTI_ARCH"
          else
            echo "Warning: Could not fetch manifest"
            echo "compressed_size=0" >> $GITHUB_OUTPUT
            echo "layer_count=0" >> $GITHUB_OUTPUT
            echo "multi_arch=false" >> $GITHUB_OUTPUT
          fi

      - name: Generate Docker metrics JSON
        run: |
          cat > docker_metrics.json << EOF
          [
            {"name": "binary/docker_image_compressed", "value": ${{ steps.docker_metrics.outputs.compressed_size || 0 }}, "unit": "bytes"},
            {"name": "binary/docker_layers", "value": ${{ steps.docker_metrics.outputs.layer_count || 0 }}, "unit": "count"}
          ]
          EOF
          echo "Docker metrics:"
          cat docker_metrics.json

      - name: Install Bencher CLI
        uses: bencherdev/bencher@main

      - name: Report Docker metrics to Bencher
        run: |
          bencher run \
            --project thoughtgate \
            --token '${{ secrets.BENCHER_API_TOKEN }}' \
            --branch releases \
            --testbed ci-ubuntu-latest \
            --adapter json \
            --file docker_metrics.json \
            || echo "Bencher report failed (non-fatal)"
        env:
          BENCHER_API_TOKEN: ${{ secrets.BENCHER_API_TOKEN }}

      - name: Generate release notes
        run: |
          VERSION="${{ steps.version.outputs.version }}"
          COMPRESSED_MB=$(echo "scale=2; ${{ steps.docker_metrics.outputs.compressed_size || 0 }} / 1048576" | bc)
          
          cat > release-notes.md << EOF
          ## Container Images
          
          Published to GitHub Container Registry:
          
          \`\`\`bash
          docker pull ghcr.io/${{ github.repository }}:${VERSION}
          docker pull ghcr.io/${{ github.repository }}:latest
          \`\`\`
          
          ## Supported Architectures
          - linux/amd64
          - linux/arm64
          
          ## Performance Metrics
          
          | Metric | Value |
          |--------|-------|
          | Image Size (compressed) | ${COMPRESSED_MB} MB |
          | Layer Count | ${{ steps.docker_metrics.outputs.layer_count }} |
          | Multi-arch | ${{ steps.docker_metrics.outputs.multi_arch }} |
          
          See full performance dashboard: [Bencher.dev](https://bencher.dev/perf/thoughtgate)
          EOF
          
          cat release-notes.md

      - name: Upload release notes
        uses: actions/upload-artifact@v6
        with:
          name: release-notes
          path: release-notes.md
          retention-days: 90

      - name: Upload Docker metrics
        uses: actions/upload-artifact@v6
        with:
          name: docker-metrics
          path: docker_metrics.json
          retention-days: 90
