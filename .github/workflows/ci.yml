name: CI/CD Pipeline

on:
  push:
    branches:
      - main
    tags:
      - 'v*'
  pull_request:
    branches:
      - '**'

env:
  CARGO_TERM_COLOR: always
  RUST_BACKTRACE: 1

jobs:
  # Stage 1a: Quick Checks (fast-fail)
  quick-checks:
    name: Format & Lint
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Install Rust toolchain
        uses: dtolnay/rust-toolchain@stable
        with:
          components: clippy, rustfmt

      - name: Cache cargo registry
        uses: Swatinem/rust-cache@v2
        with:
          shared-key: "checks"
          cache-on-failure: true

      - name: Install cargo-binstall
        uses: cargo-bins/cargo-binstall@main

      - name: Install cargo-audit
        run: |
          # Force reinstall to handle cached but missing binaries
          cargo binstall --no-confirm --force cargo-audit || cargo install --force cargo-audit
          # Verify installation
          cargo audit --version

      - name: Check formatting
        run: cargo fmt -- --check

      - name: Run clippy
        run: cargo clippy --all-targets --all-features -- -D warnings

      - name: Security audit
        run: cargo audit --deny warnings

  # Stage 1b: Build Binaries
  build:
    name: Build Binaries
    runs-on: ubuntu-latest
    needs: quick-checks
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Install Rust toolchain
        uses: dtolnay/rust-toolchain@stable

      - name: Cache cargo registry and build artifacts
        uses: Swatinem/rust-cache@v2
        with:
          shared-key: "build"
          cache-on-failure: true

      - name: Build release binaries
        run: |
          cargo build --release --bin thoughtgate
          cargo build --release --bin mock_llm --features mock

      - name: Upload binaries as artifact
        uses: actions/upload-artifact@v6
        with:
          name: binaries
          path: |
            target/release/thoughtgate
            target/release/mock_llm
          retention-days: 7

  # Stage 1c: Test Suite (parallel execution)
  test:
    name: Test (${{ matrix.test-name }})
    runs-on: ubuntu-latest
    needs: quick-checks
    
    strategy:
      fail-fast: false
      matrix:
        include:
          - test-name: "unit"
            test-cmd: "cargo test --lib -- --nocapture"
          - test-name: "streaming"
            test-cmd: "cargo test --test integration_streaming -- --nocapture"
          - test-name: "peeking"
            test-cmd: "cargo test --test unit_peeking -- --nocapture"
          - test-name: "memory"
            test-cmd: "cargo test --test memory_profile -- --nocapture"
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Install Rust toolchain
        uses: dtolnay/rust-toolchain@stable

      - name: Cache cargo registry and build artifacts
        uses: Swatinem/rust-cache@v2
        with:
          shared-key: "test-${{ matrix.test-name }}"
          cache-on-failure: true

      - name: Run ${{ matrix.test-name }} tests
        run: ${{ matrix.test-cmd }}

  # Stage 1d: Fuzzing (Smoke Tests - 60s each target)
  fuzz:
    name: Fuzz (${{ matrix.target }})
    runs-on: ubuntu-latest
    needs: quick-checks
    
    strategy:
      fail-fast: false
      matrix:
        target:
          - peeking_fuzz
          - fuzz_uri_parsing
          - fuzz_header_redaction
          - fuzz_full_request
          - fuzz_green_path       # REQ-CORE-001
          # Deferred to v0.2+:
          # - fuzz_amber_path       # REQ-CORE-002 (deferred)
          # - fuzz_inspector_chain  # REQ-CORE-002 F-004 (deferred)
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Install Rust nightly
        uses: dtolnay/rust-toolchain@nightly

      - name: Install cargo-fuzz
        run: cargo install cargo-fuzz

      - name: Restore fuzz corpus cache
        uses: actions/cache@v5
        with:
          path: fuzz/corpus/${{ matrix.target }}
          key: fuzz-corpus-${{ matrix.target }}-${{ github.sha }}
          restore-keys: |
            fuzz-corpus-${{ matrix.target }}-

      - name: Run fuzz target (60s smoke test)
        run: |
          echo "üîç Fuzzing ${{ matrix.target }} in smoke-test mode"
          echo "‚è±Ô∏è  Duration: 60s"
          
          cargo +nightly fuzz run ${{ matrix.target }} \
            -- \
            -max_total_time=60 \
            -rss_limit_mb=2048 \
            -timeout=30
        continue-on-error: false

      - name: Save corpus (even on failure)
        if: always()
        uses: actions/cache/save@v5
        with:
          path: fuzz/corpus/${{ matrix.target }}
          key: fuzz-corpus-${{ matrix.target }}-${{ github.sha }}

      - name: Upload crash artifacts
        if: failure()
        uses: actions/upload-artifact@v6
        with:
          name: fuzz-crashes-${{ matrix.target }}-${{ github.run_id }}
          path: fuzz/artifacts/${{ matrix.target }}/
          if-no-files-found: ignore
          retention-days: 30

      - name: Fail if crashes found
        if: failure()
        run: |
          echo "‚ùå Fuzzing found crashes in ${{ matrix.target }}"
          echo "üì¶ Crash inputs uploaded as artifacts"
          echo "üî¨ Reproduce with: cargo +nightly fuzz run ${{ matrix.target }} <crash-file>"
          exit 1

  # Stage 1e: Benchmarks (TTFB Performance Tracking)
  bench:
    name: Benchmarks (TTFB)
    runs-on: ubuntu-latest
    needs: quick-checks
    permissions:
      contents: write       # Needed to push benchmark results to gh-pages
      pull-requests: write  # Needed to post PR comments with benchmark comparisons
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Install Rust toolchain
        uses: dtolnay/rust-toolchain@stable

      - name: Cache cargo registry and build artifacts
        uses: Swatinem/rust-cache@v2
        with:
          shared-key: "bench"
          cache-on-failure: true

      - name: Run benchmarks
        run: |
          echo "Running TTFB benchmarks..."
          cargo bench --bench ttfb -- --save-baseline ci-benchmark
          
          echo ""
          echo "Benchmark complete. Checking output..."
          ls -la target/criterion/ || echo "Warning: target/criterion not created"
          
      - name: Parse criterion output
        run: |
          echo "Extracting criterion benchmark results..."
          
          # Debug: Show what criterion generated
          echo "=== Criterion directory structure ==="
          find target/criterion -type f -name "estimates.json" 2>/dev/null || echo "No estimates.json files found"
          
          python3 << 'PYEOF'
          import json
          from pathlib import Path
          
          def find_estimates_recursive(criterion_dir):
              """Recursively find all estimates.json files under criterion directory"""
              results = []
              
              for estimates_file in criterion_dir.rglob("estimates.json"):
                  # Get the benchmark name from the path
                  # Structure: target/criterion/<benchmark-name>/<baseline-or-new>/estimates.json
                  rel_path = estimates_file.relative_to(criterion_dir)
                  parts = rel_path.parts
                  
                  if len(parts) < 2:
                      continue
                  
                  # The benchmark name is everything except the last part (baseline dir)
                  bench_name = "/".join(parts[:-2]) if len(parts) > 2 else parts[0]
                  baseline_dir = parts[-2]  # e.g., "ci-benchmark", "new", "base"
                  
                  # Prefer ci-benchmark, then new, then base
                  priority = {"ci-benchmark": 0, "new": 1, "base": 2}.get(baseline_dir, 999)
                  
                  try:
                      with open(estimates_file) as f:
                          data = json.load(f)
                          mean_ns = data["mean"]["point_estimate"]
                          results.append({
                              "name": bench_name,
                              "baseline": baseline_dir,
                              "priority": priority,
                              "value": mean_ns,
                              "file": str(estimates_file)
                          })
                  except (KeyError, json.JSONDecodeError) as e:
                      print(f"Warning: Could not parse {estimates_file}: {e}")
                      continue
              
              return results
          
          criterion_dir = Path("target/criterion")
          
          if not criterion_dir.exists():
              print("ERROR: target/criterion not found!")
              print("Benchmark may not have run successfully.")
              exit(1)
          
          print(f"Scanning {criterion_dir} recursively...")
          
          all_results = find_estimates_recursive(criterion_dir)
          
          if not all_results:
              print("\nERROR: No benchmark results found!")
              print("Benchmark may have failed to run or produce output.")
              exit(1)
          
          print(f"\nFound {len(all_results)} benchmark result(s):")
          for r in all_results:
              print(f"  - {r['name']} ({r['baseline']}): {r['value']:.2f} ns")
          
          # Group by benchmark name and pick the best baseline (lowest priority)
          best_results = {}
          for result in all_results:
              name = result["name"]
              if name not in best_results or result["priority"] < best_results[name]["priority"]:
                  best_results[name] = result
          
          # Format for benchmark-action
          final_results = [
              {
                  "name": name,
                  "value": data["value"],
                  "unit": "ns"
              }
              for name, data in best_results.items()
          ]
          
          print(f"\nUsing {len(final_results)} benchmark result(s) for tracking:")
          for r in final_results:
              print(f"  ‚úì {r['name']}: {r['value']:.2f} {r['unit']}")
          
          # Write results
          with open("benchmark_results.json", "w") as f:
              json.dump(final_results, f, indent=2)
          
          print(f"\n‚úì Wrote benchmark_results.json")
          PYEOF
          
          echo ""
          echo "=== Final benchmark results ==="
          cat benchmark_results.json

      - name: Create gh-pages branch if needed
        if: github.event_name == 'push' && github.ref == 'refs/heads/main'
        run: |
          # Check if gh-pages branch exists remotely
          if ! git ls-remote --exit-code --heads origin gh-pages >/dev/null 2>&1; then
            echo "üìù gh-pages branch does not exist, creating..."
            
            # Create orphan branch (no commit history)
            git checkout --orphan gh-pages
            git reset --hard
            
            # Create initial commit with README
            cat > README.md << 'EOF'
          # Benchmark Results
          
          This branch contains automated benchmark results.
          
          View the dashboard at: https://${{ github.repository_owner }}.github.io/${{ github.event.repository.name }}/dev/bench/
          EOF
            
            git add README.md
            git config user.name "github-action-benchmark"
            git config user.email "github@users.noreply.github.com"
            git commit -m "Initialize gh-pages for benchmark results"
            git push origin gh-pages
            
            # Switch back to main
            git checkout main
            echo "‚úÖ gh-pages branch created"
          else
            echo "‚úÖ gh-pages branch already exists"
          fi

      - name: Store benchmark result (Push to main only)
        if: github.event_name == 'push' && github.ref == 'refs/heads/main'
        uses: benchmark-action/github-action-benchmark@v1
        with:
          tool: 'customSmallerIsBetter'
          output-file-path: benchmark_results.json
          github-token: ${{ secrets.GITHUB_TOKEN }}
          auto-push: true
          # Alert on 110% performance regression (slower)
          alert-threshold: '110%'
          comment-on-alert: true
          fail-on-alert: false
          alert-comment-cc-users: '@${{ github.repository_owner }}'

      - name: Compare benchmark result (Pull Requests)
        if: github.event_name == 'pull_request'
        uses: benchmark-action/github-action-benchmark@v1
        with:
          tool: 'customSmallerIsBetter'
          output-file-path: benchmark_results.json
          github-token: ${{ secrets.GITHUB_TOKEN }}
          auto-push: false
          # Comment with comparison on PRs
          comment-always: true
          alert-threshold: '110%'
          fail-on-alert: true

  # Stage 2: Build Container Image
  build-container:
    name: Build Container Image
    runs-on: ubuntu-latest
    needs: [build, test, fuzz]
    
    outputs:
      image-tag: ${{ steps.meta.outputs.tags }}
      
    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Generate Docker metadata
        id: meta
        run: |
          if [[ "${{ github.ref }}" == refs/tags/v* ]]; then
            VERSION=${GITHUB_REF#refs/tags/}
            echo "tags=thoughtgate:${VERSION},thoughtgate:latest" >> $GITHUB_OUTPUT
            echo "labels=version=${VERSION},commit=${GITHUB_SHA}" >> $GITHUB_OUTPUT
          elif [[ "${{ github.ref }}" == refs/heads/main ]]; then
            echo "tags=thoughtgate:main-${GITHUB_SHA::8}" >> $GITHUB_OUTPUT
            echo "labels=commit=${GITHUB_SHA}" >> $GITHUB_OUTPUT
          else
            PR_NUMBER=${{ github.event.pull_request.number }}
            echo "tags=thoughtgate:pr-${PR_NUMBER}" >> $GITHUB_OUTPUT
            echo "labels=pr=${PR_NUMBER}" >> $GITHUB_OUTPUT
          fi

      - name: Build Docker image (amd64)
        uses: docker/build-push-action@v6
        with:
          context: .
          file: ./Dockerfile
          platforms: linux/amd64
          tags: thoughtgate:test
          cache-from: type=gha
          cache-to: type=gha,mode=max
          outputs: type=docker,dest=/tmp/thoughtgate-image.tar

      - name: Upload Docker image as artifact
        uses: actions/upload-artifact@v6
        with:
          name: docker-image
          path: /tmp/thoughtgate-image.tar
          retention-days: 1

  # Stage 3: Kubernetes Integration Tests
  k8s-tests:
    name: Kubernetes Integration Tests
    runs-on: ubuntu-latest
    needs: build-container
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Install Rust toolchain
        uses: dtolnay/rust-toolchain@stable

      - name: Cache cargo registry and build artifacts
        uses: Swatinem/rust-cache@v2
        with:
          cache-on-failure: true

      - name: Download Docker image artifact
        uses: actions/download-artifact@v7
        with:
          name: docker-image
          path: /tmp

      - name: Load Docker image
        run: docker load --input /tmp/thoughtgate-image.tar

      - name: Create Kind cluster
        uses: helm/kind-action@v1.13.0
        with:
          cluster_name: kind
          wait: 30s

      - name: Load image into Kind cluster
        run: kind load docker-image thoughtgate:test --name kind

      - name: Verify image loaded in Kind
        run: |
          docker exec kind-control-plane crictl images | grep thoughtgate || \
          (echo "Failed to load image into Kind" && exit 1)

      - name: Run Kubernetes integration tests
        run: cargo test --test integration_k8s -- --nocapture

      - name: Upload performance metrics
        if: always()
        uses: actions/upload-artifact@v6
        with:
          name: k8s-performance-metrics
          path: bench_metrics.json
          if-no-files-found: ignore
          retention-days: 30

      - name: Dump Kind cluster logs on failure
        if: failure()
        run: |
          echo "=== Kind cluster info ==="
          kubectl cluster-info
          echo "=== All pods ==="
          kubectl get pods --all-namespaces
          echo "=== Kind node logs ==="
          kubectl get nodes -o wide
          docker exec kind-control-plane journalctl --no-pager || true

  # Stage 4: Publish Container (Tags Only)
  publish:
    name: Publish Container to GHCR
    runs-on: ubuntu-latest
    needs: k8s-tests
    if: startsWith(github.ref, 'refs/tags/v')
    
    permissions:
      contents: read
      packages: write
      
    steps:
      - name: Checkout code
        uses: actions/checkout@v6

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to GitHub Container Registry
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Extract version from tag
        id: version
        run: |
          VERSION=${GITHUB_REF#refs/tags/}
          echo "version=${VERSION}" >> $GITHUB_OUTPUT
          echo "Publishing version: ${VERSION}"

      - name: Generate Docker metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ghcr.io/${{ github.repository }}
          tags: |
            type=semver,pattern={{version}}
            type=semver,pattern={{major}}.{{minor}}
            type=semver,pattern={{major}}
            # Job-level condition ensures tag-only execution
            type=raw,value=latest
          labels: |
            org.opencontainers.image.title=ThoughtGate
            org.opencontainers.image.description=High-performance sidecar proxy for governing MCP and A2A agentic AI traffic
            org.opencontainers.image.vendor=${{ github.repository_owner }}
            org.opencontainers.image.version=${{ steps.version.outputs.version }}

      - name: Build and push multi-arch image
        uses: docker/build-push-action@v6
        with:
          context: .
          file: ./Dockerfile
          platforms: linux/amd64,linux/arm64
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

      - name: Generate release notes
        run: |
          echo "## Container Images" > release-notes.md
          echo "" >> release-notes.md
          echo "Published to GitHub Container Registry:" >> release-notes.md
          echo "" >> release-notes.md
          echo '```bash' >> release-notes.md
          echo "docker pull ghcr.io/${{ github.repository }}:${{ steps.version.outputs.version }}" >> release-notes.md
          echo "docker pull ghcr.io/${{ github.repository }}:latest" >> release-notes.md
          echo '```' >> release-notes.md
          echo "" >> release-notes.md
          echo "## Supported Architectures" >> release-notes.md
          echo "- linux/amd64" >> release-notes.md
          echo "- linux/arm64" >> release-notes.md
          cat release-notes.md

      - name: Upload release notes
        uses: actions/upload-artifact@v6
        with:
          name: release-notes
          path: release-notes.md
          retention-days: 90

